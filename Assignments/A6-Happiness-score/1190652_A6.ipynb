{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignemnt 6: Visualization of the Happiness Score Datasets (2015-2019)\n",
    "\n",
    "Prepared by: Sondos Aabed\n",
    "\n",
    "Student ID: 1190652\n",
    "\n",
    "Instructor: Dr. Hussien Suboh\n",
    "\n",
    "Section: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- Pipeline of the analysis process that I followed.\n",
    "- About the questions asked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Datasets (2015-2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools and Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"./happiness-score-datasets\"):\n",
    "    \"\"\"\n",
    "    Loads the data into the pandas data frame, add the year column\n",
    "    Args:\n",
    "        path (string): path to the data, deafult value is the directory name\n",
    "    Returns:\n",
    "        (list): list of data frames (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            data = pd.read_csv(path+'/'+file)\n",
    "            data['year'] = file.strip(\".csv\")\n",
    "            dfs.append(data)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for loading each dataframe separately is to first check if the columns are identical and to make data optimization using Numpy based on the datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing and Cleaning the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assess and handle Columns and Data types\n",
    "- Assess and handle Duplicates\n",
    "- Assess and handle Missing Values\n",
    "-  Assess and handle Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assessing and handling Columns and Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix structural issue to merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Inspecting the head of each of the datasets, shows incosistent and different columns names. This has to be handled for the merge step of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the renaming map that follows the convention of naming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_mapping = {\n",
    "    'Happiness.Rank': 'happiness_rank',\n",
    "    'Happiness.Score': 'happiness_score',\n",
    "    'Happiness Rank':'happiness_rank',\n",
    "    'Happiness Score':'happiness_score',\n",
    "    'Whisker.high': 'upper_confidence_interval',\n",
    "    'Upper Confidence Interval': 'upper_confidence_interval',\n",
    "    'Whisker.low': 'lower_confidence_interval',\n",
    "    'Lower Confidence Interval': 'lower_confidence_interval',\n",
    "    'Economy..GDP.per.Capita.': 'economy_gdp_per_capita',\n",
    "    'Economy (GDP per Capita)':'economy_gdp_per_capita',\n",
    "    'Health..Life.Expectancy.': 'health_life_expectancy',\n",
    "    'Trust..Government.Corruption.': 'trust_government_corruption',\n",
    "    'Dystopia.Residual': 'dystopia_residual',\n",
    "    'Dystopia Residual': 'dystopia_residual',\n",
    "    'Overall rank': 'happiness_rank',\n",
    "    'Country or region': 'country',\n",
    "    'Country':'country',\n",
    "    'Region':'region',\n",
    "    'Standard Error':'standard_error',\n",
    "    'Score': 'happiness_score',\n",
    "    'GDP per capita': 'economy_gdp_per_capita',\n",
    "    'Social support': 'family',\n",
    "    'Family':'family',\n",
    "    'Healthy life expectancy': 'health_life_expectancy',\n",
    "    'Health (Life Expectancy)': 'health_life_expectancy',\n",
    "    'Freedom to make life choices': 'freedom',\n",
    "    'Freedom': 'freedom',\n",
    "    'Perceptions of corruption': 'trust_government_corruption',\n",
    "    'Trust (Government Corruption)': 'trust_government_corruption'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_columns(dfs):\n",
    "    \"\"\"\n",
    "    Standardize the column names of the dataframes\n",
    "    Args:\n",
    "        dfs (list): list of dataframes\n",
    "    Returns:\n",
    "        (list): list of standardized (columns names) dataframes\n",
    "    \"\"\"\n",
    "    standardized_dfs = []\n",
    "    \n",
    "    for df in dfs:\n",
    "        df = df.rename(columns=rename_mapping)\n",
    "        standardized_dfs.append(df)\n",
    "    \n",
    "    return standardized_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since the datastes are standerized now we're able to concat them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat(standardize_columns(dfs), axis=0, ignore_index=True)\n",
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now the merged concatenated datsets have the shape of 782 and have 15 features. Let's work on the datatypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['year'] = pd.to_datetime(merged_df['year'], format='%Y').dt.year\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Only the year was converted to an int and the memory usage was optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## info after conversion\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assess and handle Duplicates\n",
    "Now let's check for duplicates and handle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are no duplicates records found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assess and handle Outliers\n",
    "Now let's check for outliers with visualization using boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.plot(kind='box',figsize=(15, 6));\n",
    "plt.xlabel('Columns')  \n",
    "plt.ylabel('Values') \n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.minorticks_on()\n",
    "plt.title('Boxlotting the Dataset')\n",
    "plt.tick_params(axis='x', rotation=70) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> These columns have outliers values, let's visualize the ones that does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_cols = ['Generosity', 'family','standard_error', 'trust_government_corruption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(18, 10))\n",
    "axes_flat = axes.ravel() \n",
    "\n",
    "for i, col in enumerate(outliers_cols):\n",
    "  ax = axes_flat[i] \n",
    "  merged_df[col].plot(kind='box', ax=ax)\n",
    "  ax.set_ylabel('Values')\n",
    "  ax.set_title(col + ' Boxplot')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this visualization, it is noticed that these comlumns show high numerical values, except the family column has outliers that are lowe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check for which regions have countries with extream (**outliers**) happiness score.\n",
    "Let's show the regions and the countries that has them if there is one. (Requiremnt 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['happiness_score'].plot(kind='box');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The happiness score column has no outliers on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The outliers were handled using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assess and handle Missing Values\n",
    "This is the final section of cleaning the dataset, it is about detecting and handling the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isna().sum().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It's noticed that the follwing columns have missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_vals_columns = merged_df[merged_df.columns[merged_df.isna().any()]].columns\n",
    "missing_vals_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For the trust_government_corruption column let's drop that missing value since it's only one mssing vales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For the region it will be imputed by the other value from the datasets. Since the country would always be in the same region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting the region, country pairs\n",
    "country_to_region = {}\n",
    "\n",
    "for index, row in merged_df.iterrows():\n",
    "    country = row['country']\n",
    "    region = row['region']\n",
    "    if pd.notna(region):\n",
    "        if country not in country_to_region:\n",
    "            country_to_region[country] = region\n",
    "\n",
    "print(country_to_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['region'] = merged_df.apply(lambda row: country_to_region.get(row['country'], row['region']), axis=1)\n",
    "print(\"Missing regions after imputation: \", merged_df['region'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After the applying of imputing the region, there is still 8 rows of missing values in that column so let's drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The folwing columns are dropped for having high missing values and for not being related to the task and requiremnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop_col= ['dystopia_residual', 'lower_confidence_interval', 'upper_confidence_interval','standard_error']\n",
    "merged_df.drop(columns=to_drop_col, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Finally the misisng values on the whole dataste are zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isna().sum().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highest and Lowest happiness Scores across all years\n",
    "\n",
    "Let's show the happiness scores based on the regions, aggreagted by the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.groupby('region')['happiness_score'].median().sort_values().plot(kind='barh', color='orange',  figsize=(15, 6))\n",
    "plt.title('Regions Vs. happiness scores median')\n",
    "plt.xlabel('Happiness Score')\n",
    "plt.ylabel('Regions');\n",
    "plt.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The region that has highest happiness score across all years is Australia and New Zealand.\n",
    "\n",
    "> The region that has lowest happiness score across all years is Sub-Saharan Africa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Happiness cahge Over years\n",
    "\n",
    "Let's look into the global happiness score chnage over years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.groupby('year')['happiness_score'].median().plot(kind='line', color='red', figsize=(15, 6))\n",
    "plt.xticks(merged_df['year'].unique());\n",
    "plt.title('Global happiness scores over the years')\n",
    "plt.ylabel('Happiness Score')\n",
    "plt.xlabel('Years')\n",
    "plt.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is noticed that the global happiness over the year is changing and following a positive slightly higher changes. Even though the changes are slow.\n",
    "\n",
    "> It is also noticed that in the year 2017 it got lower than the year before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The years 2020-2022 Global Happiness scores\n",
    "\n",
    "A question is raised about the next years of global happiness scores, what will the change be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Based on my experience and the context of 2020-2022 it was the yeasr were Covid-19 epedimic was rising, there had been so much suffer around the world and distrust in the goverments. I would say that the global happiness was getting lower in those next years. The econmic was going to collapse and a lot of people have lost their jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on the Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Also, based on the figure above that shows the changes over the years between (2015-2019), it is following a slight trend of going up, so based on the data the more we go by years the higher the global happiness score is going to be, it seems that 2017 was an exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's take a look into the correlation with the happiness scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.corr(numeric_only=True)[['happiness_score']].sort_values(by='happiness_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> These factors have high positive correlation with the target happiness score: \n",
    "- freedom: was contrained in quarantins while 2020-2022.\n",
    "- family\n",
    "- health_life_expectancy\n",
    "- economy_gdp_per_capita."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Happiness score change by Region\n",
    "\n",
    "Let's see how does the Happiness score change by each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_year_region = merged_df.groupby(by=['year', 'region'])['happiness_score'].median()\n",
    "unstacked_df = grouped_year_region.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstacked_df.plot(kind='line', marker='o',  figsize=(15, 6))\n",
    "plt.xlabel('Year')  \n",
    "plt.ylabel('Median Happiness Score')\n",
    "plt.title('Comparison of Median Happiness Scores by Region Over years')\n",
    "plt.xticks(rotation=45)  \n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.xticks(merged_df['year'].unique());\n",
    "plt.legend(labels=merged_df['region'].unique());\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is noticed that the regions had kept the same levels of happiness scores, which is expected if the world has been stable during the 2015-2019. Even the changes over the years were slow and slightly changing.\n",
    "\n",
    "> It is noticed that western europe had kept the same value of happiness scores. Sub-Saharan Africa had been the lowest region with happiness scores.\n",
    "\n",
    "> Even though most of the region had witnessed a lower happiness scores in 2017, the region of Central and Eastern Europe had had higher happiness scores from the year before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
